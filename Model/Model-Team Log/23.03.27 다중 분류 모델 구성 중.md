### KIEMS 방문록

이전에 작성한 거에 branch conflict를 방지하고자 브런치만 업데이트하고 붙여넣기만 하려고 했는데 다른 걸 복사해버려서 날라갔다... 굉장히 허무하다. 이런 걸 또 작성해야 한다니.

다중 분류 모델을 구성하기 전에 KIMES에 방문한 것에 대한 기록을 남기고자 한다.

코엑스의 1층과 3층에서 A~D홀로 이루어진 전시장에서 의료기기를 전시하는 박람회다. 2년 전에 KAIST에서 진단기기를 개발해서는 학술회처럼 발표하고 있는 것을 보고 올해에도 그런 팀이 있지 않을까
하는 기대를 가지고 갔었다.

C, D홀이 영상 진단기기 파트였기에 거기에서 내가 원하는 것을 볼 수 있었다. 한 기기에 대해 자세히 설명해보자면 발바닥을 통해 무게중심을 파악하며 신체도 함께 촬영하며 쏠림의 정도를 측정하는
기기였다. 그와 동시에 7여 가지의 질병을 진단하는 AI도 있었다. 개발자 분께서 직접 설명해주셨기에 이에 대해 자세히 들을 수 있었다.

의료 AI의 현 기술의 상황와 한계점에 대한 것이다. 모델에 관해서도 조금이나마 들을 수 있었다. 그 중에 이상적인 모델에 대한 내용도 여쭤봤는데, 이상적이라는 것에 대한 기준은 다르기에 병원에서
정한 것이나 논문의 내용을 바탕으로 설정한다고 했다.

이것 외에 그렇게 기억에 남는 작품은 없었다. 인공지능 기술이 발달해서 획기적인 무언가가 있을까 싶었지만 기대에 미치지는 못했다. 하지만 확실히 다른 건 이젠 인공지능의 기술이 있다며 앞세우고
있진 않다는 점이었다. 당연히 포함되어 있는 기술로 취급된 것을 알 수 있었다.

---

## 다중 분류 모델

우선 이전의 코드를 모두 실행시켜준다.

![image](https://user-images.githubusercontent.com/84713532/227820518-4adae6bc-b6c1-44ed-a964-280948e6f17e.png)

![image](https://user-images.githubusercontent.com/84713532/227820500-4197552b-6720-4217-8441-50bbe0e870b2.png)

데이터셋 구성부터 시각화까지 문제없이 오케이.. 이제 데이터를 분할시키고 모델을 구성하여 훈련을 실행하는 코드까지 완료해보겠다.


## 데이터 분할

```py
trainset, testset = random_split(trainset, [960, 240])
```

![image](https://user-images.githubusercontent.com/84713532/227823333-939a9f49-86b0-4059-9b77-690f72024994.png)

우선 trainset, testset으로 나눠봤다. 개수까지는 잘 나눠진 것을 확인할 수 있다.

![image](https://user-images.githubusercontent.com/84713532/227823385-02b91975-c4ba-4632-8fe8-026d937a7c54.png)

더 다양하게 시각화를 해봤다. 문제점 하나를 발견했다. 이미지는 분명 adress같아보이는데 라벨은 5(follow)라고 한다. json file부터가 잘못 기입되었고, 이걸 확인하지 않고 분류된대로
폴더에 넣어 정리해서 생긴 문제점이다..

얼마 해보지도 않았는데 이런 라벨링 결과가 보이니 조금 걱정스럽다. 하지만 모델 간의 성능을 비교하는 것이기에 큰 문제는 되지 않을 거라 생각하고 싶다.

이제 testset에서 validationset으로 더 나누어서 8:1:1의 비율로 960:120:120으로 나누겠다.

random_split을 이용하니 잘 섞이는 것은 의심치 않겠다.

```py
testset, valset = random_split(testset, [120, 120])
```

![image](https://user-images.githubusercontent.com/84713532/227824155-c7774f7c-0148-45f6-bfb0-0ca5b62c5a12.png)

잘 나눠졌다. cell을 비효율적으로 사용하고 있는 것 같아서 같은 cell로 옮겨서 다시 해보려고 하니까 오류가 발생했다. 이미 120개로 나눠진 set에서 또 나누려고 해서 그런 것 같다.

코드만 옮겨놓고 정리해서 실행은 시키지 말아야겠다.

![image](https://user-images.githubusercontent.com/84713532/227824469-523d79c7-8094-4d4d-92ea-38880937202b.png)

이것도 시각화 한번 시켜봤는데 이거 이미지 상태가 좀 심각하다. 너무 진해져서 나오는데 지금은 시각화를 위해 permute를 사용했는데, 모델 학습시킬 때에도 이게 필요한 건지 검토해보고 필요하다면 수정해야겠다. 내가 봐서는 무슨 동작인지 모르겠지만 downswing이라고 라벨값이 붙어있긴 하다.

```py
figure, axes = plt.subplots(nrows=1, ncols=8, figsize=(22, 6))
for i in range(8):
  axes[i].imshow(trainset[i][0].permute(1, 2, 0), cmap='gray')
  axes[i].set_title(trainset[i][1])
```

![image](https://user-images.githubusercontent.com/84713532/227824906-3f599a42-2ab2-424a-83c9-9fa23ec62605.png)

어쩌면 json value가 생각보다 더 난장판이 아닌가 싶다.

```py
labels_map = {0 : 'adress', 1 : 'backswing', 2: 'backswingtop', 3: 'downswing', 4: 'finish', 5: 'follow', 6: 'impact', 7: 'takeback'}

figure, axes = plt.subplots(nrows=4, ncols=8, figsize=(16, 8))
axes = axes.flatten()

for i in range(32):
    rand_i = np.random.randint(0, len(trainset))
    image = trainset[rand_i][0].permute(1, 2, 0)
    axes[i].axis('off')
    axes[i].imshow(image)
    axes[i].set_title(labels_map[trainset[rand_i][1]])   
```

label 값을 숫자에서 이름으로 바꿔주는 과정에서 

```
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
```

이미지 한장당 이런 경고가 계속 발생하는 것을 유심히 보게 되었다. 보니까 정규화된 값을 RGB의 0~255값으로 변경하여 출력하는 것 같았다.
이걸 보고 어쩌면 정규화 때문에 이런 문제가 발생한 게 아닌가 싶었다.

그럼 이걸 해결하기 위한 방안은 두 가지가 있다.

- 정규화를 하지 않는 것.
- 정규화한 이미지를 미리 픽셀값 0~255 범위의 RGB 이미지로 바꿔놓는 것이다.

근데 막상 이걸 적고나니까 정규화는 모델 훈련을 위해 해주는 작업이다. 중요한 건 시각화가 아니라 모델 훈련이다.

원래 모델 훈련 직전에 정규화를 시켜주는데, 정규화된 이미지를 출력하다보니 이런 문제가 발생한 것 같다. (AI는 이런 사진들을 보고 뭘 훈련한다는 걸까)
어쨌든 시각화는 마무리 하겠다.

---

## 모델 구성

모델 부분은 구성 방법과 종류가 매우 다양하기 때문에 많은 실험을 할 수 있다. 그렇기에 기록은 필수적인 요소이다.

#### 우선 구성 방법

- 전이학습
- 범용적인 CNN 모델 뼈대 직접 구성
- 개인적인 모델 구성하기

#### 모델 종류

- ResNet
- GoogleNet
- VGGNet
- EfficientNet
- and so on

위의 여러가지 방법들이 있는데 먼저 준비해두었던 Roboflow에서 제공하는 fast.ai의 모델을 활용해보겠다. Resnet과 EfficientNet이 있던 것 같은데, 비교해보겠다.

```py
!pip install fastai
```

![image](https://user-images.githubusercontent.com/84713532/227827861-73d029f1-bda7-465e-b7c1-d2e1f3b1a6b9.png)

뭐가 이렇게 많아.. 일단 오케이

```py
from fastai.vision import *
```

```py
#follow the link below to get your download code from from Roboflow
!pip install -q roboflow
from roboflow import Roboflow
rf = Roboflow(model_format="folder", notebook="roboflow-resnet")
```

![image](https://user-images.githubusercontent.com/84713532/227827934-388b8075-6482-401a-8637-d13178b83cf9.png)

문제 발생 - API 키가 옳지 않은 형태라고 한다.

![image](https://user-images.githubusercontent.com/84713532/227828148-d8faf789-52fa-4fa7-b966-fa08aa45c7b3.png)

str type이여야 하는데 Nonetype이라고 한다. 이걸 내가 해결할 수 있는 문제인가..?
구글링을 해봐야겠다.

[참고 링크](https://github.com/facebookresearch/detectron2/issues/3649)

문제점을 살펴보니 Environment, 즉, 버전 문제다. 왜 항상 뭐 좀 써보려고 하면 버전이 문제일까.. 다른 선택지가 많은 상황에서 이렇게 버전을 굳이 바꿔가면서 하고 싶진 않은데..
Roboflow 사이트 들어가서 더 찾아봐야겠다.

![image](https://user-images.githubusercontent.com/84713532/227828935-66baed88-b60d-4316-accf-1e505a0f0aef.png)

찾아보니 비록 Yolo5에 관한 내용이지만 Roboflow의 버전을 바꾸는 방법이 있다는 걸 알아냈다.

나도 ResNet을 사용하기 위한 버전을 찾아보거나 모든 버전을 한번씩 모두 실행해보는 방법을 취해야겠다.

하지만 아래로 좀 더 내려보니 버전을 바꿔도 문제는 여전하다고 했고, 이 문제를 해결하기 위해선 jupyter notebook에서 데이터를 처리하고 zip file type으로 바꾼 다음 구글 colab에 올려주면 된다는 내용이었다. 복잡하다.. 그냥 데이터 받아서 쓰도록 만들었어야 되는 거 아닌가. 좋지 않다.

Roboflow 사이트에서 코드를 참고하러 들어갔는데, 여기도 Teachable Machine처럼 데이터만 입력하면 모델을 구성해주는 서비스가 있는 것을 발견했다.

그래서 여기서 이미지를 훈련시켰는데, 입력 데이터에 라벨값이 붙은 이미지가 아니었어서 결과는 확인할 수 없었다. 근데 training data prediction이 95%던데 어떤 걸 기준으로 했길래 이런 건지는 의문이다. 어쨌든 이 모델을 다운로드해서 사용할 수 있다고 하니 ResNet 모델도 안되는 김에 한번 사용해봐야겠다.


- 데이터 훈련 결과 이미지

![image](https://user-images.githubusercontent.com/84713532/227854746-423c360c-3b1e-4389-a79d-79c6b4c040c1.png)

데이터 적재부터 전처리에 훈련까지 버튼만 누르면 완료할 수 있다.

모델을 Export 버튼으로 가져와서 1200장짜리 데이터를 훈련시켜 사용해보겠다.

```py
!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="yfORKOy5rb7MScH5HNpJ")
project = rf.workspace("ujs").project("multiclassification-tcg2a")
dataset = project.version(1).download("multiclass")
```

모델을 다운로드하는 코드다. 

```
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting roboflow
  Downloading roboflow-1.0.1-py3-none-any.whl (55 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.7/55.7 KB 7.1 MB/s eta 0:00:00
Collecting idna==2.10
  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 KB 7.9 MB/s eta 0:00:00
Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.65.0)
Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.7.0.72)
Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.4.4)
Collecting cycler==0.10.0
  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)
Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.22.4)
Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from roboflow) (8.4.0)
Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.27.1)
Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.26.15)
Collecting requests-toolbelt
  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 KB 7.3 MB/s eta 0:00:00
Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from roboflow) (6.0)
Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.16.0)
Collecting python-dotenv
  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)
Collecting chardet==4.0.0
  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.7/178.7 KB 21.2 MB/s eta 0:00:00
Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.9/dist-packages (from roboflow) (2022.12.7)
Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from roboflow) (3.7.1)
Collecting wget
  Downloading wget-3.2.zip (10 kB)
  Preparing metadata (setup.py) ... done
Collecting pyparsing==2.4.7
  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.8/67.8 KB 8.5 MB/s eta 0:00:00
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.8.2)
Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (5.12.0)
Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (4.39.2)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (23.0)
Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (1.0.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->roboflow) (2.0.12)
Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->roboflow) (3.15.0)
Building wheels for collected packages: wget
  Building wheel for wget (setup.py) ... done
  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=30e9650751862d82352487964b630a70d35a46bf6411fdc61448d67bab02af91
  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38
Successfully built wget
Installing collected packages: wget, python-dotenv, pyparsing, idna, cycler, chardet, requests-toolbelt, roboflow
  Attempting uninstall: pyparsing
    Found existing installation: pyparsing 3.0.9
    Uninstalling pyparsing-3.0.9:
      Successfully uninstalled pyparsing-3.0.9
  Attempting uninstall: idna
    Found existing installation: idna 3.4
    Uninstalling idna-3.4:
      Successfully uninstalled idna-3.4
  Attempting uninstall: cycler
    Found existing installation: cycler 0.11.0
    Uninstalling cycler-0.11.0:
      Successfully uninstalled cycler-0.11.0
  Attempting uninstall: chardet
    Found existing installation: chardet 3.0.4
    Uninstalling chardet-3.0.4:
      Successfully uninstalled chardet-3.0.4
Successfully installed chardet-4.0.0 cycler-0.10.0 idna-2.10 pyparsing-2.4.7 python-dotenv-1.0.0 requests-toolbelt-0.10.1 roboflow-1.0.1 wget-3.2
WARNING: The following packages were previously imported in this runtime:
  [cycler,pyparsing]
You must restart the runtime in order to use newly installed versions.
loading Roboflow workspace...
loading Roboflow project...
Downloading Dataset Version Zip in Multiclassification-1 to multiclass: 100% [142950651 / 142950651] bytes
Extracting Dataset Version Zip to Multiclassification-1 in multiclass:: 100%|██████████| 2163/2163 [00:01<00:00, 1951.54it/s]
```

WARNING이 뜨긴 했지만 괜찮다. 실행은 됐으니까.

다음으로는 resnet34 모델을 위해 구성된 코드를 가져와서 그대로 사용해보겠다.

```py
dataset.location
```

```py
#build fastai dataset loader
np.random.seed(42)
#fastai automatically factors the ./train and ./valid folders into seperate datasets
#more details https://docs.fast.ai/vision.data.html#ImageDataLoaders.from_folder
path = Path(dataset.location)
data = ImageDataBunch.from_folder(path, size=224, num_workers=4).normalize(imagenet_stats)
```

![image](https://user-images.githubusercontent.com/84713532/227856071-2e77c3ea-033a-4e45-8832-e1fadbde0191.png)

여기서 오류는 왜 뜨는교..? 뭔가 모듈이 잘못됐거나 install이 안된 거 같다.

해결하고자

```py
!pip install -q roboflow
```

이 코드도 실행했는데 달라진 건 없었다. 그럼 코드를 수정하는 과정에서 누락된 게 있을 거란 추측을 하게 되었다. 코드 원본을 보러가보자.

해당 코드를 살펴봤는데 결과도 뚜렷하지 않고 왜 다른 좋은 코드들 많은데 이 코드를 옮겨적어놨는지 모르겠다. 다른 코드를 사용하도록 하겠다.

---

### [해당 코드](https://github.com/verrannt/Tutorials/blob/master/fastai_plant_seedlings_classification.ipynb)

```py
from fastai.vision import *
from fastai.metrics import accuracy
```

```py
path = "./plant-seedlings-data/"
size = 224
bs = 64
```

path 설정을 잘해줘야 할 것 같은데 어떻게 하면 좋을지 전체 코드를 보고 판단해야겠다.

살펴보고 오니 path를 잘 설정해줘도 data에서 한번 더 막힌다. fast.ai에서 구성한 데이터를 가져오게 되어 있어서 여기서 train, test, validation으로 나누지도 않고 모두 다운로드만 해서 사용한다. 이 코드를 사용하기엔 무리가 있을 것 같다.

그럼 이제 남은 건 내가 직접 모델을 구성하거나 전이학습을 하는 것이다. Let's get it!

---

### 모델 뼈대 구성

```py
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torch.optim as optim
import copy
import sys, time
from torch.autograd import Variable
```

#### 우선 모델 구성을 위한 모듈 import 추가적으로 해주고

```py
class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


def ResNet18():
    return ResNet(BasicBlock, [2,2,2,2])

def ResNet34():
    return ResNet(BasicBlock, [3,4,6,3])

def ResNet50():
    return ResNet(Bottleneck, [3,4,6,3])

def ResNet101():
    return ResNet(Bottleneck, [3,4,23,3])

def ResNet152():
    return ResNet(Bottleneck, [3,8,36,3])


def test():
    net = ResNet101()
    y = net(torch.randn(1,3,32,32))
    print(y.size())
```

```py
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
```

```py
model = ResNet101()
model.to(device)
model
```

```
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (11): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (17): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (18): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (19): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (20): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (21): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (22): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (linear): Linear(in_features=2048, out_features=10, bias=True)
)
```

```py
for parameter in model.parameters():
  print(parameter.size())
```

```
torch.Size([64, 3, 3, 3])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 1, 1])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 3, 3])
torch.Size([64])
torch.Size([64])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([64, 256, 1, 1])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 3, 3])
torch.Size([64])
torch.Size([64])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([64, 256, 1, 1])
torch.Size([64])
torch.Size([64])
torch.Size([64, 64, 3, 3])
torch.Size([64])
torch.Size([64])
torch.Size([256, 64, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([128, 256, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 256, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([128, 512, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([128, 512, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([128, 512, 1, 1])
torch.Size([128])
torch.Size([128])
torch.Size([128, 128, 3, 3])
torch.Size([128])
torch.Size([128])
torch.Size([512, 128, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([256, 512, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([1024, 512, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([256, 1024, 1, 1])
torch.Size([256])
torch.Size([256])
torch.Size([256, 256, 3, 3])
torch.Size([256])
torch.Size([256])
torch.Size([1024, 256, 1, 1])
torch.Size([1024])
torch.Size([1024])
torch.Size([512, 1024, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 512, 3, 3])
torch.Size([512])
torch.Size([512])
torch.Size([2048, 512, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([2048, 1024, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([512, 2048, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 512, 3, 3])
torch.Size([512])
torch.Size([512])
torch.Size([2048, 512, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([512, 2048, 1, 1])
torch.Size([512])
torch.Size([512])
torch.Size([512, 512, 3, 3])
torch.Size([512])
torch.Size([512])
torch.Size([2048, 512, 1, 1])
torch.Size([2048])
torch.Size([2048])
torch.Size([10, 2048])
torch.Size([10])
```

```py
for name, parameter in model.named_parameters():
  print(name, parameter.size())
```

```
conv1.weight torch.Size([64, 3, 3, 3])
bn1.weight torch.Size([64])
bn1.bias torch.Size([64])
layer1.0.conv1.weight torch.Size([64, 64, 1, 1])
layer1.0.bn1.weight torch.Size([64])
layer1.0.bn1.bias torch.Size([64])
layer1.0.conv2.weight torch.Size([64, 64, 3, 3])
layer1.0.bn2.weight torch.Size([64])
layer1.0.bn2.bias torch.Size([64])
layer1.0.conv3.weight torch.Size([256, 64, 1, 1])
layer1.0.bn3.weight torch.Size([256])
layer1.0.bn3.bias torch.Size([256])
layer1.0.shortcut.0.weight torch.Size([256, 64, 1, 1])
layer1.0.shortcut.1.weight torch.Size([256])
layer1.0.shortcut.1.bias torch.Size([256])
layer1.1.conv1.weight torch.Size([64, 256, 1, 1])
layer1.1.bn1.weight torch.Size([64])
layer1.1.bn1.bias torch.Size([64])
layer1.1.conv2.weight torch.Size([64, 64, 3, 3])
layer1.1.bn2.weight torch.Size([64])
layer1.1.bn2.bias torch.Size([64])
layer1.1.conv3.weight torch.Size([256, 64, 1, 1])
layer1.1.bn3.weight torch.Size([256])
layer1.1.bn3.bias torch.Size([256])
layer1.2.conv1.weight torch.Size([64, 256, 1, 1])
layer1.2.bn1.weight torch.Size([64])
layer1.2.bn1.bias torch.Size([64])
layer1.2.conv2.weight torch.Size([64, 64, 3, 3])
layer1.2.bn2.weight torch.Size([64])
layer1.2.bn2.bias torch.Size([64])
layer1.2.conv3.weight torch.Size([256, 64, 1, 1])
layer1.2.bn3.weight torch.Size([256])
layer1.2.bn3.bias torch.Size([256])
layer2.0.conv1.weight torch.Size([128, 256, 1, 1])
layer2.0.bn1.weight torch.Size([128])
layer2.0.bn1.bias torch.Size([128])
layer2.0.conv2.weight torch.Size([128, 128, 3, 3])
layer2.0.bn2.weight torch.Size([128])
layer2.0.bn2.bias torch.Size([128])
layer2.0.conv3.weight torch.Size([512, 128, 1, 1])
layer2.0.bn3.weight torch.Size([512])
layer2.0.bn3.bias torch.Size([512])
layer2.0.shortcut.0.weight torch.Size([512, 256, 1, 1])
layer2.0.shortcut.1.weight torch.Size([512])
layer2.0.shortcut.1.bias torch.Size([512])
layer2.1.conv1.weight torch.Size([128, 512, 1, 1])
layer2.1.bn1.weight torch.Size([128])
layer2.1.bn1.bias torch.Size([128])
layer2.1.conv2.weight torch.Size([128, 128, 3, 3])
layer2.1.bn2.weight torch.Size([128])
layer2.1.bn2.bias torch.Size([128])
layer2.1.conv3.weight torch.Size([512, 128, 1, 1])
layer2.1.bn3.weight torch.Size([512])
layer2.1.bn3.bias torch.Size([512])
layer2.2.conv1.weight torch.Size([128, 512, 1, 1])
layer2.2.bn1.weight torch.Size([128])
layer2.2.bn1.bias torch.Size([128])
layer2.2.conv2.weight torch.Size([128, 128, 3, 3])
layer2.2.bn2.weight torch.Size([128])
layer2.2.bn2.bias torch.Size([128])
layer2.2.conv3.weight torch.Size([512, 128, 1, 1])
layer2.2.bn3.weight torch.Size([512])
layer2.2.bn3.bias torch.Size([512])
layer2.3.conv1.weight torch.Size([128, 512, 1, 1])
layer2.3.bn1.weight torch.Size([128])
layer2.3.bn1.bias torch.Size([128])
layer2.3.conv2.weight torch.Size([128, 128, 3, 3])
layer2.3.bn2.weight torch.Size([128])
layer2.3.bn2.bias torch.Size([128])
layer2.3.conv3.weight torch.Size([512, 128, 1, 1])
layer2.3.bn3.weight torch.Size([512])
layer2.3.bn3.bias torch.Size([512])
layer3.0.conv1.weight torch.Size([256, 512, 1, 1])
layer3.0.bn1.weight torch.Size([256])
layer3.0.bn1.bias torch.Size([256])
layer3.0.conv2.weight torch.Size([256, 256, 3, 3])
layer3.0.bn2.weight torch.Size([256])
layer3.0.bn2.bias torch.Size([256])
layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.0.bn3.weight torch.Size([1024])
layer3.0.bn3.bias torch.Size([1024])
layer3.0.shortcut.0.weight torch.Size([1024, 512, 1, 1])
layer3.0.shortcut.1.weight torch.Size([1024])
layer3.0.shortcut.1.bias torch.Size([1024])
layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.1.bn1.weight torch.Size([256])
layer3.1.bn1.bias torch.Size([256])
layer3.1.conv2.weight torch.Size([256, 256, 3, 3])
layer3.1.bn2.weight torch.Size([256])
layer3.1.bn2.bias torch.Size([256])
layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.1.bn3.weight torch.Size([1024])
layer3.1.bn3.bias torch.Size([1024])
layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.2.bn1.weight torch.Size([256])
layer3.2.bn1.bias torch.Size([256])
layer3.2.conv2.weight torch.Size([256, 256, 3, 3])
layer3.2.bn2.weight torch.Size([256])
layer3.2.bn2.bias torch.Size([256])
layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.2.bn3.weight torch.Size([1024])
layer3.2.bn3.bias torch.Size([1024])
layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.3.bn1.weight torch.Size([256])
layer3.3.bn1.bias torch.Size([256])
layer3.3.conv2.weight torch.Size([256, 256, 3, 3])
layer3.3.bn2.weight torch.Size([256])
layer3.3.bn2.bias torch.Size([256])
layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.3.bn3.weight torch.Size([1024])
layer3.3.bn3.bias torch.Size([1024])
layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.4.bn1.weight torch.Size([256])
layer3.4.bn1.bias torch.Size([256])
layer3.4.conv2.weight torch.Size([256, 256, 3, 3])
layer3.4.bn2.weight torch.Size([256])
layer3.4.bn2.bias torch.Size([256])
layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.4.bn3.weight torch.Size([1024])
layer3.4.bn3.bias torch.Size([1024])
layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.5.bn1.weight torch.Size([256])
layer3.5.bn1.bias torch.Size([256])
layer3.5.conv2.weight torch.Size([256, 256, 3, 3])
layer3.5.bn2.weight torch.Size([256])
layer3.5.bn2.bias torch.Size([256])
layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.5.bn3.weight torch.Size([1024])
layer3.5.bn3.bias torch.Size([1024])
layer3.6.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.6.bn1.weight torch.Size([256])
layer3.6.bn1.bias torch.Size([256])
layer3.6.conv2.weight torch.Size([256, 256, 3, 3])
layer3.6.bn2.weight torch.Size([256])
layer3.6.bn2.bias torch.Size([256])
layer3.6.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.6.bn3.weight torch.Size([1024])
layer3.6.bn3.bias torch.Size([1024])
layer3.7.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.7.bn1.weight torch.Size([256])
layer3.7.bn1.bias torch.Size([256])
layer3.7.conv2.weight torch.Size([256, 256, 3, 3])
layer3.7.bn2.weight torch.Size([256])
layer3.7.bn2.bias torch.Size([256])
layer3.7.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.7.bn3.weight torch.Size([1024])
layer3.7.bn3.bias torch.Size([1024])
layer3.8.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.8.bn1.weight torch.Size([256])
layer3.8.bn1.bias torch.Size([256])
layer3.8.conv2.weight torch.Size([256, 256, 3, 3])
layer3.8.bn2.weight torch.Size([256])
layer3.8.bn2.bias torch.Size([256])
layer3.8.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.8.bn3.weight torch.Size([1024])
layer3.8.bn3.bias torch.Size([1024])
layer3.9.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.9.bn1.weight torch.Size([256])
layer3.9.bn1.bias torch.Size([256])
layer3.9.conv2.weight torch.Size([256, 256, 3, 3])
layer3.9.bn2.weight torch.Size([256])
layer3.9.bn2.bias torch.Size([256])
layer3.9.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.9.bn3.weight torch.Size([1024])
layer3.9.bn3.bias torch.Size([1024])
layer3.10.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.10.bn1.weight torch.Size([256])
layer3.10.bn1.bias torch.Size([256])
layer3.10.conv2.weight torch.Size([256, 256, 3, 3])
layer3.10.bn2.weight torch.Size([256])
layer3.10.bn2.bias torch.Size([256])
layer3.10.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.10.bn3.weight torch.Size([1024])
layer3.10.bn3.bias torch.Size([1024])
layer3.11.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.11.bn1.weight torch.Size([256])
layer3.11.bn1.bias torch.Size([256])
layer3.11.conv2.weight torch.Size([256, 256, 3, 3])
layer3.11.bn2.weight torch.Size([256])
layer3.11.bn2.bias torch.Size([256])
layer3.11.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.11.bn3.weight torch.Size([1024])
layer3.11.bn3.bias torch.Size([1024])
layer3.12.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.12.bn1.weight torch.Size([256])
layer3.12.bn1.bias torch.Size([256])
layer3.12.conv2.weight torch.Size([256, 256, 3, 3])
layer3.12.bn2.weight torch.Size([256])
layer3.12.bn2.bias torch.Size([256])
layer3.12.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.12.bn3.weight torch.Size([1024])
layer3.12.bn3.bias torch.Size([1024])
layer3.13.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.13.bn1.weight torch.Size([256])
layer3.13.bn1.bias torch.Size([256])
layer3.13.conv2.weight torch.Size([256, 256, 3, 3])
layer3.13.bn2.weight torch.Size([256])
layer3.13.bn2.bias torch.Size([256])
layer3.13.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.13.bn3.weight torch.Size([1024])
layer3.13.bn3.bias torch.Size([1024])
layer3.14.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.14.bn1.weight torch.Size([256])
layer3.14.bn1.bias torch.Size([256])
layer3.14.conv2.weight torch.Size([256, 256, 3, 3])
layer3.14.bn2.weight torch.Size([256])
layer3.14.bn2.bias torch.Size([256])
layer3.14.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.14.bn3.weight torch.Size([1024])
layer3.14.bn3.bias torch.Size([1024])
layer3.15.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.15.bn1.weight torch.Size([256])
layer3.15.bn1.bias torch.Size([256])
layer3.15.conv2.weight torch.Size([256, 256, 3, 3])
layer3.15.bn2.weight torch.Size([256])
layer3.15.bn2.bias torch.Size([256])
layer3.15.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.15.bn3.weight torch.Size([1024])
layer3.15.bn3.bias torch.Size([1024])
layer3.16.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.16.bn1.weight torch.Size([256])
layer3.16.bn1.bias torch.Size([256])
layer3.16.conv2.weight torch.Size([256, 256, 3, 3])
layer3.16.bn2.weight torch.Size([256])
layer3.16.bn2.bias torch.Size([256])
layer3.16.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.16.bn3.weight torch.Size([1024])
layer3.16.bn3.bias torch.Size([1024])
layer3.17.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.17.bn1.weight torch.Size([256])
layer3.17.bn1.bias torch.Size([256])
layer3.17.conv2.weight torch.Size([256, 256, 3, 3])
layer3.17.bn2.weight torch.Size([256])
layer3.17.bn2.bias torch.Size([256])
layer3.17.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.17.bn3.weight torch.Size([1024])
layer3.17.bn3.bias torch.Size([1024])
layer3.18.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.18.bn1.weight torch.Size([256])
layer3.18.bn1.bias torch.Size([256])
layer3.18.conv2.weight torch.Size([256, 256, 3, 3])
layer3.18.bn2.weight torch.Size([256])
layer3.18.bn2.bias torch.Size([256])
layer3.18.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.18.bn3.weight torch.Size([1024])
layer3.18.bn3.bias torch.Size([1024])
layer3.19.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.19.bn1.weight torch.Size([256])
layer3.19.bn1.bias torch.Size([256])
layer3.19.conv2.weight torch.Size([256, 256, 3, 3])
layer3.19.bn2.weight torch.Size([256])
layer3.19.bn2.bias torch.Size([256])
layer3.19.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.19.bn3.weight torch.Size([1024])
layer3.19.bn3.bias torch.Size([1024])
layer3.20.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.20.bn1.weight torch.Size([256])
layer3.20.bn1.bias torch.Size([256])
layer3.20.conv2.weight torch.Size([256, 256, 3, 3])
layer3.20.bn2.weight torch.Size([256])
layer3.20.bn2.bias torch.Size([256])
layer3.20.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.20.bn3.weight torch.Size([1024])
layer3.20.bn3.bias torch.Size([1024])
layer3.21.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.21.bn1.weight torch.Size([256])
layer3.21.bn1.bias torch.Size([256])
layer3.21.conv2.weight torch.Size([256, 256, 3, 3])
layer3.21.bn2.weight torch.Size([256])
layer3.21.bn2.bias torch.Size([256])
layer3.21.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.21.bn3.weight torch.Size([1024])
layer3.21.bn3.bias torch.Size([1024])
layer3.22.conv1.weight torch.Size([256, 1024, 1, 1])
layer3.22.bn1.weight torch.Size([256])
layer3.22.bn1.bias torch.Size([256])
layer3.22.conv2.weight torch.Size([256, 256, 3, 3])
layer3.22.bn2.weight torch.Size([256])
layer3.22.bn2.bias torch.Size([256])
layer3.22.conv3.weight torch.Size([1024, 256, 1, 1])
layer3.22.bn3.weight torch.Size([1024])
layer3.22.bn3.bias torch.Size([1024])
layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])
layer4.0.bn1.weight torch.Size([512])
layer4.0.bn1.bias torch.Size([512])
layer4.0.conv2.weight torch.Size([512, 512, 3, 3])
layer4.0.bn2.weight torch.Size([512])
layer4.0.bn2.bias torch.Size([512])
layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])
layer4.0.bn3.weight torch.Size([2048])
layer4.0.bn3.bias torch.Size([2048])
layer4.0.shortcut.0.weight torch.Size([2048, 1024, 1, 1])
layer4.0.shortcut.1.weight torch.Size([2048])
layer4.0.shortcut.1.bias torch.Size([2048])
layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])
layer4.1.bn1.weight torch.Size([512])
layer4.1.bn1.bias torch.Size([512])
layer4.1.conv2.weight torch.Size([512, 512, 3, 3])
layer4.1.bn2.weight torch.Size([512])
layer4.1.bn2.bias torch.Size([512])
layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])
layer4.1.bn3.weight torch.Size([2048])
layer4.1.bn3.bias torch.Size([2048])
layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])
layer4.2.bn1.weight torch.Size([512])
layer4.2.bn1.bias torch.Size([512])
layer4.2.conv2.weight torch.Size([512, 512, 3, 3])
layer4.2.bn2.weight torch.Size([512])
layer4.2.bn2.bias torch.Size([512])
layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])
layer4.2.bn3.weight torch.Size([2048])
layer4.2.bn3.bias torch.Size([2048])
linear.weight torch.Size([10, 2048])
linear.bias torch.Size([10])
```

#### 층과 파라미터 합치기 성공

```py
from torchsummary import summary
```

```py
summary(model, (3, 224, 224)) # (channel, input_size)
```

갑자기 이건 왜 안돼?

![image](https://user-images.githubusercontent.com/84713532/227862519-c3d9031f-09a1-422e-b8b2-190c3d4ef167.png)

요약 뭐 잘 돼있겠지~ 이거 못 본다고 실행 안되는 거 아니다! pass!

---

### Model Compile

```py
learning_rate = 0.001 
# 손실함수
criterion = nn.CrossEntropyLoss()
# 옵티마이저(경사하강법, 최적화 함수)
#optimizer = optim.SGD(model.parameters(), lr = learning_rate)
# 규제의 강도 설정 weight_decay
# optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=0.001)
optimizer = optim.Adam(model.parameters(), lr = learning_rate)
```

```py
# Learning Rate Schedule
# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html

#monitoring 하고 있는 값(valid_loss)이 patience 기간동안(onPlateau) 줄어들지 않을때 lr에 factor(0.1)를 곱해준다.
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.1, verbose=True)
```

하이퍼 파라미터를 설정해줬다. 모델 간의 성능 비교가 목적이라 파라미터가 어느정도 잘 설정되어 있기 때문에 굳이 건드리지 않겠다.

---

### Model Training

```py
def validation(model, validloader, criterion):
  # 전방향 예측후 나온 점수(logits)의 최대값을 최종 예측으로 준비
  # 이 최종 예측과 정답을 비교
  # 전체 중 맞은 것의 개수 비율을 정확도(accuracy)로 계산
  valid_accuracy = 0
  valid_loss = 0

  # 전방향 예측을 구할 때는 gradient가 필요가 없음
  with torch.no_grad():
    for images, labels in validloader: # 10000개의 데이터에 대해 16개씩(미니배치 사이즈) 10000/16번을 iterations
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28
      
      # 2. 전방향(Forward) 예측 
      logits = model.forward(images) # 점수 반환
      _, preds = torch.max(logits, 1) # 16개에 대한 최종 예측
      # preds= probs.max(dim=1)[1] 
      correct = (preds == labels).sum()

      accuracy = correct / images.shape[0]
      loss = criterion(logits, labels) # 16개에 대한 loss
      
      valid_accuracy += accuracy
      valid_loss += loss.item() # tensor 값을 꺼내옴
    

  return valid_loss, valid_accuracy # validloader 전체 대한 총 loss, 총 accuracy
```

```py
from torch.utils.tensorboard import SummaryWriter

writer  = SummaryWriter()
```

```py
def train(model, epochs, criterion, optimizer):
  steps = 0
  min_loss = 10000
  max_accuracy = 0

  trigger = 0
  patience = 13 # for Early stopping

  # 1 에폭(epoch)당 반복수
  #steps_per_epoch = len(trainset)/batch_size # 2500 iterations
  steps_per_epoch = len(trainloader) # 2500 iterations

  for epoch in range(epochs):
    model.train()
    train_loss = 0
    for images, labels in iter(trainloader): # 이터레이터로부터 미니배치 16개씩을 가져와 images, labels에 준비
      steps += 1
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력 데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28

      # 2. 전방향(Forward) 예측 
      outputs = model.forward(images) # 예측
      loss = criterion(outputs, labels) # 예측과 결과를 통해 Cross Entropy Loss 반환

      # 3. 역방향(Backward) 오차(Gradient) 전파
      optimizer.zero_grad() # 파이토치에서 gradient가 누적되지 않게 하기 위해
      loss.backward()

      # 4. 경사하강법으로 모델 파라미터 업데이트
      optimizer.step() # W <- W -lr*Gradient

      train_loss += loss.item()
      if (steps % steps_per_epoch) == 0: # step : 2500, .... (epoch 마다)
        model.eval() # 배치 정규화, 드롭아웃이 적용될 때는 model.forward 연산이 training때와 다르므로 반드시 설정
        valid_loss, valid_accuracy = validation(model, valset, criterion)

        # tensorboad 시각화를 위한 로그 이벤트 등록
        writer.add_scalar("Loss/train", train_loss/len(trainset), epoch)
        writer.add_scalar("Loss/valid", valid_loss/len(valset), epoch)
        writer.add_scalars("Loss/train and valid",
                          {'train' : train_loss/len(trainset),
                          'valid' : valid_loss/len(valset)}, epoch)
        
        writer.add_scalar("Valid Accuracy", valid_accuracy/len(valset), epoch)


        print('Epoch : {}/{}.....'.format(epoch+1, epochs),
              'Train Loss : {:.3f}'.format(train_loss/len(trainset)),
              'Valid Loss : {:.3f}'.format(valid_loss/len(valset)),
              'Valid Accuracy : {:.3f}'.format(valid_accuracy/len(valset)))
        
        # Best model 저장
        # option 1
        # if valid_loss < min_loss:
        #   min_loss = valid_loss
        #   torch.save(model.state_dict(), 'best_checkpoint.pth')

        # option 2
        if valid_accuracy > max_accuracy: 
          max_accuracy = valid_accuracy
          torch.save(model.state_dict(), 'best_checkpoint.pth')

        # Early Stopping (조기 종료)
        if valid_loss > min_loss:
          trigger += 1 # valid loss가 min_loss 를 갱신하지 못할때마다 증가
          print('trigger : ', trigger )
          if trigger > patience:
            print('Early Stopping!!!')
            print('Traning step is finished!!')
            writer.flush()  
            return   
        else:
          trigger = 0
          min_loss = valid_loss


        train_loss = 0
        model.train()

        # Learning Rate Scheduler
        scheduler.step(valid_loss)

  writer.flush()   
```

#### 이제 훈련만 시키면 끝!!

```py
epochs=15
train(model, epochs, criterion, optimizer)
```

- 실행결과

```
OutOfMemoryError                          Traceback (most recent call last)
<ipython-input-75-0115010f288c> in <module>
      1 epochs=15
----> 2 train(model, epochs, criterion, optimizer)

6 frames
/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py in relu(input, inplace)
   1455         result = torch.relu_(input)
   1456     else:
-> 1457         result = torch.relu(input)
   1458     return result
   1459 

OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB (GPU 0; 14.75 GiB total capacity; 11.77 GiB already allocated; 676.81 MiB free; 12.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

이건 또 뭔 에러야... 여기서 다른 코드로 넘어가기엔 너무 아쉽다 구글링으로 원인을 찾아보자.

생각보다 심각한 에러다. 초과된 메모리 공간이라는 뜻인데 난 12GB까지 가능한데 14GB가 필요하다고 한다. 복잡하지만 다양한 해결 방안들이 있으니 차근차근 해보겠다.

```py
import torch, gc
gc.collect()
torch.cuda.empty_cache()
```

우선 캐시 삭제를 해봤지만 결과는 같았다.

아 이거 혹시 구글 드라이브 용량 말하는 건가? 14.75GB 이 숫자... 맞는 거 같은데 드라이브 더 정리해봐야겠다.

내 사진이나 동영상은 혹시라도 하나라도 사라지는 게 있을까봐 안지우려고 했는데...파일 다운로드 해놓은 것도 있고, 네이버 클라우드도 있으니까 믿고 지워봐야겠다.....

![image](https://user-images.githubusercontent.com/84713532/227867137-35980c6a-dafe-4686-b536-248530f86eb1.png)

동영상들 몇 기가 정도 분량이나 삭제했는데 왜 그대로야...

![image](https://user-images.githubusercontent.com/84713532/227867226-4951fe8a-068a-4e02-a1b5-ea014f75a614.png)

시간 지나니까 해결됐다! 이제 750MB 정도는 충분히 할당될만한 여유가 생겼다. 실행해보자.

이게 문제가 아니었나보다.... 여전히 안된다. 다른 방법들을 시도해보겠다..

```
It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved.
```

배치사이즈 때문에 생겼던 오류로 낮추니 해결됐다고 한다. 해보겠다.

확인해보니까 이미 사이즈는 16이다. 8로도 되려나.. 더 올리는 것도 해봐야겠다.

배치사이즈는 4로 하니까 해결되었다.

근데 바로 입출력 데이터의 크기가 달라서 생기는 오류가 발생한다. 이런 오류는 다음과 같은 경우에 생긴다고 한다.

- 입력과 출력 데이터의 크기가 다를 때
- 흑백만 처리하도록 되어있는 코드에 컬러 이미지를 처리하라고 할 때
- 층이 많아지면서 생긴 문제일 수도 있다.
- pytorch 버전 문제
