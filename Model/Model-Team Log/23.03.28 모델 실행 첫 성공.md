28일까지 이 모델만큼은 완성하기로 했는데...이제 마지막 날이다. 다중 분류 모델 완성돼도 동엽님이 알아오신 프레임 나누는 거랑 결합시키고 거기서 모델이 동작별로 포착을 해내야 한다.
이걸 오늘안에??ㅋㅋㅋㅋㅋ

#### 어쨌든 어제 막혔던 부분에서 이어서 시작하도록 하겠다.

mat1과 mat2의 크기가 같지 않아서 생긴 문제였다. torchsummary에서도 오류가 났었지만 이전에 jupyter notebook에서도 오류가 났었고, 이게 실행이 안된다 해도 훈련은 될 거라는 생각에 넘겼는데 
그렇게 넘기면 안되는 문제였다.

층 간의 크기가 맞지 않다고 하니 이미지의 크기나 모델 구성부터 확인하도록 하겠다.

## 입출력 크기 오류 수정 작업

#### 우선 출력을 10에서 8로 바꿔줬다.

- 변경 전: num_classes=10

```py
class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
```

- 변경 후: num_classes=8

```py
class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=8):
```

#### resnet도 101층에서 34층으로 줄여줬다.

그랬더니 오류내용이 조금 바꼈다.

RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x25088 and 512x8)

층의 크기가 조금 바꼈다. 하지만 여전히 오류인 건 변함없다.

뭐가 잘못된 건지 찾다가 class 자체가 잘못된 건가 했지만 문제는 없었다.

저 곱하기에 대한 문제를 계속 고민해봤다. 그러다가 summary 코드를 건드려봤다.

```py
summary(model, (3, 64, 64)) # (channel, input_size)
```

수가 계속 바뀌었다.

```
RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x8192 and 2048x8)
```

수많은 시도 끝에 이 정도까지 맞춰놨다. 둘의 계산 결과는 16384로 같다. 하지만 둘이 결합시킬 수가 없댄다. 그래서 한층 더 낮춰봤다.

```py
summary(model, (3, 32, 32)) # (channel, input_size)
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
            Conv2d-3           [-1, 64, 32, 32]           4,096
       BatchNorm2d-4           [-1, 64, 32, 32]             128
            Conv2d-5           [-1, 64, 32, 32]          36,864
       BatchNorm2d-6           [-1, 64, 32, 32]             128
            Conv2d-7          [-1, 256, 32, 32]          16,384
       BatchNorm2d-8          [-1, 256, 32, 32]             512
            Conv2d-9          [-1, 256, 32, 32]          16,384
      BatchNorm2d-10          [-1, 256, 32, 32]             512
       Bottleneck-11          [-1, 256, 32, 32]               0
           Conv2d-12           [-1, 64, 32, 32]          16,384
      BatchNorm2d-13           [-1, 64, 32, 32]             128
           Conv2d-14           [-1, 64, 32, 32]          36,864
      BatchNorm2d-15           [-1, 64, 32, 32]             128
           Conv2d-16          [-1, 256, 32, 32]          16,384
      BatchNorm2d-17          [-1, 256, 32, 32]             512
       Bottleneck-18          [-1, 256, 32, 32]               0
           Conv2d-19           [-1, 64, 32, 32]          16,384
      BatchNorm2d-20           [-1, 64, 32, 32]             128
           Conv2d-21           [-1, 64, 32, 32]          36,864
      BatchNorm2d-22           [-1, 64, 32, 32]             128
           Conv2d-23          [-1, 256, 32, 32]          16,384
      BatchNorm2d-24          [-1, 256, 32, 32]             512
       Bottleneck-25          [-1, 256, 32, 32]               0
           Conv2d-26          [-1, 128, 32, 32]          32,768
      BatchNorm2d-27          [-1, 128, 32, 32]             256
           Conv2d-28          [-1, 128, 16, 16]         147,456
      BatchNorm2d-29          [-1, 128, 16, 16]             256
           Conv2d-30          [-1, 512, 16, 16]          65,536
      BatchNorm2d-31          [-1, 512, 16, 16]           1,024
           Conv2d-32          [-1, 512, 16, 16]         131,072
      BatchNorm2d-33          [-1, 512, 16, 16]           1,024
       Bottleneck-34          [-1, 512, 16, 16]               0
           Conv2d-35          [-1, 128, 16, 16]          65,536
      BatchNorm2d-36          [-1, 128, 16, 16]             256
           Conv2d-37          [-1, 128, 16, 16]         147,456
      BatchNorm2d-38          [-1, 128, 16, 16]             256
           Conv2d-39          [-1, 512, 16, 16]          65,536
      BatchNorm2d-40          [-1, 512, 16, 16]           1,024
       Bottleneck-41          [-1, 512, 16, 16]               0
           Conv2d-42          [-1, 128, 16, 16]          65,536
      BatchNorm2d-43          [-1, 128, 16, 16]             256
           Conv2d-44          [-1, 128, 16, 16]         147,456
      BatchNorm2d-45          [-1, 128, 16, 16]             256
           Conv2d-46          [-1, 512, 16, 16]          65,536
      BatchNorm2d-47          [-1, 512, 16, 16]           1,024
       Bottleneck-48          [-1, 512, 16, 16]               0
           Conv2d-49          [-1, 128, 16, 16]          65,536
      BatchNorm2d-50          [-1, 128, 16, 16]             256
           Conv2d-51          [-1, 128, 16, 16]         147,456
      BatchNorm2d-52          [-1, 128, 16, 16]             256
           Conv2d-53          [-1, 512, 16, 16]          65,536
      BatchNorm2d-54          [-1, 512, 16, 16]           1,024
       Bottleneck-55          [-1, 512, 16, 16]               0
           Conv2d-56          [-1, 256, 16, 16]         131,072
      BatchNorm2d-57          [-1, 256, 16, 16]             512
           Conv2d-58            [-1, 256, 8, 8]         589,824
      BatchNorm2d-59            [-1, 256, 8, 8]             512
           Conv2d-60           [-1, 1024, 8, 8]         262,144
      BatchNorm2d-61           [-1, 1024, 8, 8]           2,048
           Conv2d-62           [-1, 1024, 8, 8]         524,288
      BatchNorm2d-63           [-1, 1024, 8, 8]           2,048
       Bottleneck-64           [-1, 1024, 8, 8]               0
           Conv2d-65            [-1, 256, 8, 8]         262,144
      BatchNorm2d-66            [-1, 256, 8, 8]             512
           Conv2d-67            [-1, 256, 8, 8]         589,824
      BatchNorm2d-68            [-1, 256, 8, 8]             512
           Conv2d-69           [-1, 1024, 8, 8]         262,144
      BatchNorm2d-70           [-1, 1024, 8, 8]           2,048
       Bottleneck-71           [-1, 1024, 8, 8]               0
           Conv2d-72            [-1, 256, 8, 8]         262,144
      BatchNorm2d-73            [-1, 256, 8, 8]             512
           Conv2d-74            [-1, 256, 8, 8]         589,824
      BatchNorm2d-75            [-1, 256, 8, 8]             512
           Conv2d-76           [-1, 1024, 8, 8]         262,144
      BatchNorm2d-77           [-1, 1024, 8, 8]           2,048
       Bottleneck-78           [-1, 1024, 8, 8]               0
           Conv2d-79            [-1, 256, 8, 8]         262,144
      BatchNorm2d-80            [-1, 256, 8, 8]             512
           Conv2d-81            [-1, 256, 8, 8]         589,824
      BatchNorm2d-82            [-1, 256, 8, 8]             512
           Conv2d-83           [-1, 1024, 8, 8]         262,144
      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048
       Bottleneck-85           [-1, 1024, 8, 8]               0
           Conv2d-86            [-1, 256, 8, 8]         262,144
      BatchNorm2d-87            [-1, 256, 8, 8]             512
           Conv2d-88            [-1, 256, 8, 8]         589,824
      BatchNorm2d-89            [-1, 256, 8, 8]             512
           Conv2d-90           [-1, 1024, 8, 8]         262,144
      BatchNorm2d-91           [-1, 1024, 8, 8]           2,048
       Bottleneck-92           [-1, 1024, 8, 8]               0
           Conv2d-93            [-1, 256, 8, 8]         262,144
      BatchNorm2d-94            [-1, 256, 8, 8]             512
           Conv2d-95            [-1, 256, 8, 8]         589,824
      BatchNorm2d-96            [-1, 256, 8, 8]             512
           Conv2d-97           [-1, 1024, 8, 8]         262,144
      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048
       Bottleneck-99           [-1, 1024, 8, 8]               0
          Conv2d-100            [-1, 256, 8, 8]         262,144
     BatchNorm2d-101            [-1, 256, 8, 8]             512
          Conv2d-102            [-1, 256, 8, 8]         589,824
     BatchNorm2d-103            [-1, 256, 8, 8]             512
          Conv2d-104           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-105           [-1, 1024, 8, 8]           2,048
      Bottleneck-106           [-1, 1024, 8, 8]               0
          Conv2d-107            [-1, 256, 8, 8]         262,144
     BatchNorm2d-108            [-1, 256, 8, 8]             512
          Conv2d-109            [-1, 256, 8, 8]         589,824
     BatchNorm2d-110            [-1, 256, 8, 8]             512
          Conv2d-111           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-112           [-1, 1024, 8, 8]           2,048
      Bottleneck-113           [-1, 1024, 8, 8]               0
          Conv2d-114            [-1, 256, 8, 8]         262,144
     BatchNorm2d-115            [-1, 256, 8, 8]             512
          Conv2d-116            [-1, 256, 8, 8]         589,824
     BatchNorm2d-117            [-1, 256, 8, 8]             512
          Conv2d-118           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-119           [-1, 1024, 8, 8]           2,048
      Bottleneck-120           [-1, 1024, 8, 8]               0
          Conv2d-121            [-1, 256, 8, 8]         262,144
     BatchNorm2d-122            [-1, 256, 8, 8]             512
          Conv2d-123            [-1, 256, 8, 8]         589,824
     BatchNorm2d-124            [-1, 256, 8, 8]             512
          Conv2d-125           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-126           [-1, 1024, 8, 8]           2,048
      Bottleneck-127           [-1, 1024, 8, 8]               0
          Conv2d-128            [-1, 256, 8, 8]         262,144
     BatchNorm2d-129            [-1, 256, 8, 8]             512
          Conv2d-130            [-1, 256, 8, 8]         589,824
     BatchNorm2d-131            [-1, 256, 8, 8]             512
          Conv2d-132           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-133           [-1, 1024, 8, 8]           2,048
      Bottleneck-134           [-1, 1024, 8, 8]               0
          Conv2d-135            [-1, 256, 8, 8]         262,144
     BatchNorm2d-136            [-1, 256, 8, 8]             512
          Conv2d-137            [-1, 256, 8, 8]         589,824
     BatchNorm2d-138            [-1, 256, 8, 8]             512
          Conv2d-139           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-140           [-1, 1024, 8, 8]           2,048
      Bottleneck-141           [-1, 1024, 8, 8]               0
          Conv2d-142            [-1, 256, 8, 8]         262,144
     BatchNorm2d-143            [-1, 256, 8, 8]             512
          Conv2d-144            [-1, 256, 8, 8]         589,824
     BatchNorm2d-145            [-1, 256, 8, 8]             512
          Conv2d-146           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-147           [-1, 1024, 8, 8]           2,048
      Bottleneck-148           [-1, 1024, 8, 8]               0
          Conv2d-149            [-1, 256, 8, 8]         262,144
     BatchNorm2d-150            [-1, 256, 8, 8]             512
          Conv2d-151            [-1, 256, 8, 8]         589,824
     BatchNorm2d-152            [-1, 256, 8, 8]             512
          Conv2d-153           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-154           [-1, 1024, 8, 8]           2,048
      Bottleneck-155           [-1, 1024, 8, 8]               0
          Conv2d-156            [-1, 256, 8, 8]         262,144
     BatchNorm2d-157            [-1, 256, 8, 8]             512
          Conv2d-158            [-1, 256, 8, 8]         589,824
     BatchNorm2d-159            [-1, 256, 8, 8]             512
          Conv2d-160           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-161           [-1, 1024, 8, 8]           2,048
      Bottleneck-162           [-1, 1024, 8, 8]               0
          Conv2d-163            [-1, 256, 8, 8]         262,144
     BatchNorm2d-164            [-1, 256, 8, 8]             512
          Conv2d-165            [-1, 256, 8, 8]         589,824
     BatchNorm2d-166            [-1, 256, 8, 8]             512
          Conv2d-167           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-168           [-1, 1024, 8, 8]           2,048
      Bottleneck-169           [-1, 1024, 8, 8]               0
          Conv2d-170            [-1, 256, 8, 8]         262,144
     BatchNorm2d-171            [-1, 256, 8, 8]             512
          Conv2d-172            [-1, 256, 8, 8]         589,824
     BatchNorm2d-173            [-1, 256, 8, 8]             512
          Conv2d-174           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-175           [-1, 1024, 8, 8]           2,048
      Bottleneck-176           [-1, 1024, 8, 8]               0
          Conv2d-177            [-1, 256, 8, 8]         262,144
     BatchNorm2d-178            [-1, 256, 8, 8]             512
          Conv2d-179            [-1, 256, 8, 8]         589,824
     BatchNorm2d-180            [-1, 256, 8, 8]             512
          Conv2d-181           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-182           [-1, 1024, 8, 8]           2,048
      Bottleneck-183           [-1, 1024, 8, 8]               0
          Conv2d-184            [-1, 256, 8, 8]         262,144
     BatchNorm2d-185            [-1, 256, 8, 8]             512
          Conv2d-186            [-1, 256, 8, 8]         589,824
     BatchNorm2d-187            [-1, 256, 8, 8]             512
          Conv2d-188           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-189           [-1, 1024, 8, 8]           2,048
      Bottleneck-190           [-1, 1024, 8, 8]               0
          Conv2d-191            [-1, 256, 8, 8]         262,144
     BatchNorm2d-192            [-1, 256, 8, 8]             512
          Conv2d-193            [-1, 256, 8, 8]         589,824
     BatchNorm2d-194            [-1, 256, 8, 8]             512
          Conv2d-195           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-196           [-1, 1024, 8, 8]           2,048
      Bottleneck-197           [-1, 1024, 8, 8]               0
          Conv2d-198            [-1, 256, 8, 8]         262,144
     BatchNorm2d-199            [-1, 256, 8, 8]             512
          Conv2d-200            [-1, 256, 8, 8]         589,824
     BatchNorm2d-201            [-1, 256, 8, 8]             512
          Conv2d-202           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-203           [-1, 1024, 8, 8]           2,048
      Bottleneck-204           [-1, 1024, 8, 8]               0
          Conv2d-205            [-1, 256, 8, 8]         262,144
     BatchNorm2d-206            [-1, 256, 8, 8]             512
          Conv2d-207            [-1, 256, 8, 8]         589,824
     BatchNorm2d-208            [-1, 256, 8, 8]             512
          Conv2d-209           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-210           [-1, 1024, 8, 8]           2,048
      Bottleneck-211           [-1, 1024, 8, 8]               0
          Conv2d-212            [-1, 256, 8, 8]         262,144
     BatchNorm2d-213            [-1, 256, 8, 8]             512
          Conv2d-214            [-1, 256, 8, 8]         589,824
     BatchNorm2d-215            [-1, 256, 8, 8]             512
          Conv2d-216           [-1, 1024, 8, 8]         262,144
     BatchNorm2d-217           [-1, 1024, 8, 8]           2,048
      Bottleneck-218           [-1, 1024, 8, 8]               0
          Conv2d-219            [-1, 512, 8, 8]         524,288
     BatchNorm2d-220            [-1, 512, 8, 8]           1,024
          Conv2d-221            [-1, 512, 4, 4]       2,359,296
     BatchNorm2d-222            [-1, 512, 4, 4]           1,024
          Conv2d-223           [-1, 2048, 4, 4]       1,048,576
     BatchNorm2d-224           [-1, 2048, 4, 4]           4,096
          Conv2d-225           [-1, 2048, 4, 4]       2,097,152
     BatchNorm2d-226           [-1, 2048, 4, 4]           4,096
      Bottleneck-227           [-1, 2048, 4, 4]               0
          Conv2d-228            [-1, 512, 4, 4]       1,048,576
     BatchNorm2d-229            [-1, 512, 4, 4]           1,024
          Conv2d-230            [-1, 512, 4, 4]       2,359,296
     BatchNorm2d-231            [-1, 512, 4, 4]           1,024
          Conv2d-232           [-1, 2048, 4, 4]       1,048,576
     BatchNorm2d-233           [-1, 2048, 4, 4]           4,096
      Bottleneck-234           [-1, 2048, 4, 4]               0
          Conv2d-235            [-1, 512, 4, 4]       1,048,576
     BatchNorm2d-236            [-1, 512, 4, 4]           1,024
          Conv2d-237            [-1, 512, 4, 4]       2,359,296
     BatchNorm2d-238            [-1, 512, 4, 4]           1,024
          Conv2d-239           [-1, 2048, 4, 4]       1,048,576
     BatchNorm2d-240           [-1, 2048, 4, 4]           4,096
      Bottleneck-241           [-1, 2048, 4, 4]               0
          Linear-242                    [-1, 8]          16,392
================================================================
Total params: 42,508,872
Trainable params: 42,508,872
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 100.13
Params size (MB): 162.16
Estimated Total Size (MB): 262.30
----------------------------------------------------------------
```

됐다.... 이제 이걸로 훈련까지 될까?    돼? 되? 라...

![image](https://user-images.githubusercontent.com/84713532/228100876-befd49d6-98d5-4cd4-939c-69e98255d493.png)

여기서 다시 막혔다... 이번엔 4X100352란다. 100352는 224X224X2의 결과값이다. 244는 이미지의 높이 너비의 값이라 치지만 2랑 4는 뭘까?

다른 코드도 가져와보고 실행해보다가 이미지 크기를 32X32로 맞추고 실행해봤다. 크기 문제는 해결된 것 같은데

![image](https://user-images.githubusercontent.com/84713532/228101841-64023db8-c654-4a80-8b67-9e072ebb761b.png)

다시 이 문제로 돌아왔다. 램공간이 부족한 거 같은데 이런 경우엔 어떻게 해야할까. 이미 batch_size=4인데, 데이터를 다시 가져와볼까?

```
OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.75 GiB total capacity; 13.38 GiB already allocated; 832.00 KiB free; 13.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

오류 내용을 보니 이미 할당된 메모리가 있다고 한다. 그럼 해결책은 캐시삭제다. 삭제하고 다시 실행해보고자 한다.


```py
epochs=15
train(model, epochs, criterion, optimizer)
```

다시 이 코드를 실행해봤다. 약 1분 20초 동안 실행됐다. 이제 좀 되나 싶었는데

![image](https://user-images.githubusercontent.com/84713532/228103211-4f1c5bee-e414-47bf-be26-b2ba94a2f663.png)

어제 맞닥드린 문제와 다시 만났다. 이전의 오류들은 해결된 거라 믿고 다시 실행해보겠다. 근데 된다해도 32X32 짜리 이미지로 뭘 할 수 있을까...

저건 좀 뒤로 미루고 다른 코드를 실행시켜봤다.

```py
TrainModel(model, criterion=criterion, optimizer=optimizer,trainloader=trainloader,testloader=validloader,num_epochs=10)
```

```
Epoch 1/10
train | Loss: 2.1527 Acc: 0.1719
test | Loss: 2.4020 Acc: 0.2167
Epoch 2/10
train | Loss: 2.0940 Acc: 0.1646
test | Loss: 3.2324 Acc: 0.1833
Epoch 3/10
train | Loss: 2.0948 Acc: 0.1771
test | Loss: 2.6611 Acc: 0.1167
Epoch 4/10
train | Loss: 2.0488 Acc: 0.1740
test | Loss: 3.2925 Acc: 0.2250
Epoch 5/10
train | Loss: 2.0135 Acc: 0.1906
test | Loss: 2.5711 Acc: 0.1917
Epoch 6/10
train | Loss: 2.0107 Acc: 0.2042
test | Loss: 2.2332 Acc: 0.2417
Epoch 7/10
train | Loss: 2.0328 Acc: 0.2000
test | Loss: 2.6443 Acc: 0.1167
Epoch 8/10
train | Loss: 2.0496 Acc: 0.2021
test | Loss: 2.3014 Acc: 0.1583
Epoch 9/10
train | Loss: 2.0473 Acc: 0.1854
test | Loss: 3.3143 Acc: 0.2000
Epoch 10/10
train | Loss: 2.1094 Acc: 0.1677
test | Loss: 2.2243 Acc: 0.2583
<__main__.TrainModel at 0x7f13af794b20>
```

얘는 실행됐다ㅋㅋㅋㅋㅋㅋㅋ근데 성능이ㅋㅋㅋㅋㅋㅋㅋㅋㅋ재밌다.

32X32 짜리인데다가 각 클래스에 제대로 된 사진들이 들어있는 것도 아니다. 이 성능이 어느 정도 또 이해되기도 한다. 실행되는 게 어딘가 싶기도 하다. 

다른 코드 더 찾아보고 안되면 어쩔 수 없다... 있는 걸 쓸 수 밖에

```py
def test(model,testloader,criterion):
        model.eval()
        test_loss = 0
        correct = 0
        total = 0
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(testloader):
                inputs,targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets.long())     

                test_loss += loss.data.cpu().numpy()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
                
            epoch_loss = test_loss / len(testloader)
            epoch_acc = correct / total
            print('test | Loss: {:.4f} Acc: {:.4f}'.format( epoch_loss, epoch_acc))

test(model,testloader,criterion)
```

```
test | Loss: 2.4169 Acc: 0.2083
```

test 성능ㅋㅋㅋㅋㅋㅋㅋㅋㅋ20퍼센트라고 한다. 그래도 32픽셀 짜리로 이상하게 모인 데이터 가지고 8개의 클래스 중에 20%나 맞춘 거면 괜찮은데..?
