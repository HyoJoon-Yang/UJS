#### 어제 데이터 정리와 전이학습의 뛰어난 성과를 확인했다. 오늘은 여러 성능을 검토하고 동엽님의 코드를 통해 프레임으로 나눠 이미지 분류 후 동작별로 한장씩 추출해내는 코드를 짜보겠다.

## ResNet_Transfer-Learning 층별 성능 비교

### Pretrained=True, epoch=10

#### ResNet18

```
Epoch 1/10
train | Loss: 2.8206 Acc: 0.2156
test | Loss: 1.9134 Acc: 0.2250
Epoch 2/10
train | Loss: 1.8365 Acc: 0.3167
test | Loss: 1.6853 Acc: 0.3167
Epoch 3/10
train | Loss: 1.7864 Acc: 0.3615
test | Loss: 1.9682 Acc: 0.3083
Epoch 4/10
train | Loss: 1.5198 Acc: 0.4458
test | Loss: 1.2613 Acc: 0.4833
Epoch 5/10
train | Loss: 1.1698 Acc: 0.5490
test | Loss: 1.2369 Acc: 0.5750
Epoch 6/10
train | Loss: 0.9545 Acc: 0.6500
test | Loss: 1.2754 Acc: 0.5917
Epoch 7/10
train | Loss: 0.6859 Acc: 0.7417
test | Loss: 2.0830 Acc: 0.4750
Epoch 8/10
train | Loss: 0.5448 Acc: 0.8187
test | Loss: 0.3727 Acc: 0.8333
Epoch 9/10
train | Loss: 0.4524 Acc: 0.8396
test | Loss: 0.3322 Acc: 0.8667
Epoch 10/10
train | Loss: 0.3769 Acc: 0.8677
test | Loss: 0.2496 Acc: 0.9000
<__main__.TrainModel at 0x7fa6f0e6f700>
```

```
test | Loss: 0.1554 Acc: 0.9667
```


#### ResNet34

```
Epoch 1/10
train | Loss: 2.4241 Acc: 0.2490
test | Loss: 2.4633 Acc: 0.3583
Epoch 2/10
train | Loss: 1.7506 Acc: 0.3490
test | Loss: 4.6523 Acc: 0.2000
Epoch 3/10
train | Loss: 1.5298 Acc: 0.3937
test | Loss: 1.6087 Acc: 0.3667
Epoch 4/10
train | Loss: 1.2978 Acc: 0.4865
test | Loss: 1.2053 Acc: 0.4333
Epoch 5/10
train | Loss: 1.0723 Acc: 0.5552
test | Loss: 1.4976 Acc: 0.4833
Epoch 6/10
train | Loss: 1.0083 Acc: 0.5990
test | Loss: 1.9423 Acc: 0.3667
Epoch 7/10
train | Loss: 0.9097 Acc: 0.6177
test | Loss: 1.2716 Acc: 0.5167
Epoch 8/10
train | Loss: 0.7472 Acc: 0.7073
test | Loss: 0.8846 Acc: 0.6583
Epoch 9/10
train | Loss: 0.6439 Acc: 0.7458
test | Loss: 1.1313 Acc: 0.6500
Epoch 10/10
train | Loss: 0.5018 Acc: 0.8073
test | Loss: 0.4734 Acc: 0.8167
<__main__.TrainModel at 0x7f0ab475c250>
```

```
test | Loss: 0.3721 Acc: 0.8667
```

이전 데이터셋에 비해 성능이 좋다. 층이 깊어짐에 비해 epoch가 적어서 떨어지는 정확도의 범위가 확연히 줄었다.


#### ResNet50

```
Epoch 1/10
train | Loss: 2.4912 Acc: 0.2156
test | Loss: 4.6013 Acc: 0.1583
Epoch 2/10
train | Loss: 1.9665 Acc: 0.2115
test | Loss: 1.8232 Acc: 0.3000
Epoch 3/10
train | Loss: 1.7726 Acc: 0.2979
test | Loss: 1.7157 Acc: 0.2667
Epoch 4/10
train | Loss: 1.6100 Acc: 0.3438
test | Loss: 1.3409 Acc: 0.4250
Epoch 5/10
train | Loss: 1.4585 Acc: 0.4188
test | Loss: 1.4790 Acc: 0.4417
Epoch 6/10
train | Loss: 1.4217 Acc: 0.4354
test | Loss: 1.0461 Acc: 0.5583
Epoch 7/10
train | Loss: 1.2642 Acc: 0.4958
test | Loss: 1.3905 Acc: 0.4750
Epoch 8/10
train | Loss: 1.0692 Acc: 0.5500
test | Loss: 0.9891 Acc: 0.5833
Epoch 9/10
train | Loss: 0.9375 Acc: 0.6375
test | Loss: 1.0761 Acc: 0.5833
Epoch 10/10
train | Loss: 0.8256 Acc: 0.6760
test | Loss: 1.1789 Acc: 0.5667
<__main__.TrainModel at 0x7f0ab46f5700>
```

```
test | Loss: 0.8464 Acc: 0.6667
```

- 층의 깊이와 epoch간의 관계를 파악하고 있는 지금, 더 이상 깊은 층을 해보는 건 시간낭비다. pretrained=False로 다시 성능을 검토해보겠다.
---

### Pretrained=False, epoch=10

#### ResNet18

```
Epoch 1/10
train | Loss: 2.4576 Acc: 0.2156
test | Loss: 1.8601 Acc: 0.3833
Epoch 2/10
train | Loss: 1.7892 Acc: 0.2479
test | Loss: 2.0932 Acc: 0.1500
Epoch 3/10
train | Loss: 1.7513 Acc: 0.3052
test | Loss: 1.4866 Acc: 0.4500
Epoch 4/10
train | Loss: 1.5537 Acc: 0.3937
test | Loss: 1.4076 Acc: 0.3750
Epoch 5/10
train | Loss: 1.3685 Acc: 0.4573
test | Loss: 1.3748 Acc: 0.5000
Epoch 6/10
train | Loss: 1.2484 Acc: 0.5021
test | Loss: 1.2819 Acc: 0.5167
Epoch 7/10
train | Loss: 1.0511 Acc: 0.5563
test | Loss: 0.9555 Acc: 0.6167
Epoch 8/10
train | Loss: 0.9392 Acc: 0.5969
test | Loss: 0.7507 Acc: 0.6167
Epoch 9/10
train | Loss: 0.8338 Acc: 0.6521
test | Loss: 0.9594 Acc: 0.5750
Epoch 10/10
train | Loss: 0.6789 Acc: 0.6927
test | Loss: 0.7011 Acc: 0.6500
<__main__.TrainModel at 0x7f0ab45b9130>
```

```
test | Loss: 0.5336 Acc: 0.7333
```

Pretrain의 효과는 굉장하다.. 오차는 약 3.5배, 정확률은 20% 정도가 차이난다.


#### ResNet34

```
Epoch 1/10
train | Loss: 2.4513 Acc: 0.1802
test | Loss: 2.2113 Acc: 0.2917
Epoch 2/10
train | Loss: 1.9846 Acc: 0.2000
test | Loss: 1.7927 Acc: 0.2833
Epoch 3/10
train | Loss: 1.8387 Acc: 0.2552
test | Loss: 2.0456 Acc: 0.3083
Epoch 4/10
train | Loss: 1.6951 Acc: 0.3104
test | Loss: 1.5471 Acc: 0.3667
Epoch 5/10
train | Loss: 1.6448 Acc: 0.3333
test | Loss: 1.5582 Acc: 0.3083
Epoch 6/10
train | Loss: 1.5376 Acc: 0.3604
test | Loss: 1.3756 Acc: 0.4417
Epoch 7/10
train | Loss: 1.4329 Acc: 0.3990
test | Loss: 1.2105 Acc: 0.4500
Epoch 8/10
train | Loss: 1.3813 Acc: 0.3979
test | Loss: 1.1496 Acc: 0.5167
Epoch 9/10
train | Loss: 1.2961 Acc: 0.4594
test | Loss: 1.0489 Acc: 0.5667
Epoch 10/10
train | Loss: 1.1302 Acc: 0.5240
test | Loss: 0.9880 Acc: 0.6167
<__main__.TrainModel at 0x7f0a805cc760>
```

```
test | Loss: 0.8427 Acc: 0.6833
```

- pretrain의 효과는 확실히 알게 되었고 층이 깊어짐에 따라 성능이 저하되는 것 또한 같다는 것도 알게 되었다.

#### 이제 성능이 가장 좋은 ResNet18(pretrained=True) 모델을 가지고 결과를 시각화해서 볼 예정이다.

## Test 결과 시각화

#### 우선 이미지를 testloader에서 가져와서 검증해보겠다.

```py
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
```

```py
test_iter = iter(validloader)
images, labels = next(test_iter)
images, labels = images.to(device), labels.to(device)
print(images.size(), labels.size())
```

```py
rnd_idx = 6
images[rnd_idx:rnd_idx+1].shape, labels[rnd_idx:rnd_idx+1] # 1, 3, 32, 32
```

```py
# not Flatten!
# flattend_img = images[rnd_idx].view(1, 784)
img = images[rnd_idx:rnd_idx+1]
with torch.no_grad():
  model.eval() # 배치 정규화가 들어가면서 전방향 연산이 학습시와는 달라지므로 반드시 eval() 넣어야 함
  logit = model.forward(img)

pred = logit.max(dim=1)[1]
pred == labels[rnd_idx]
```

```py
pred, labels[rnd_idx]
```

```py
img = img.cpu()
plt.imshow(img[0].permute(1, 2, 0)) 
```

![image](https://user-images.githubusercontent.com/84713532/228404936-ae7ff019-5763-4321-8137-7d3c4f7beb23.png)

정답: 3(finish) / 예측 결과: 3(finish)

- 근데 문제가 발생했다.

test | Loss: 1.5784 Acc: 0.4000

성능이 갑자기 반토막 이상으로 나버린 것이다.

pretrain=True 상태인데... 데이터셋도 확인해봤지만 달라진 건 없었다. 수치만 이렇게 변한 것이다.

데이터셋 구성부터 다시 실행했더니 0.5917으로 조금은 상승했다. 성능이 이렇게 나와버리면 안되는데...

![image](https://user-images.githubusercontent.com/84713532/228407372-4444b84f-34e4-4432-b584-b21f9b5bf2d8.png)

그래도 이건 계속 잘 맞춘다.

아직 안한 전처리 기술도 있으니 성능은 올라갈 거라 믿고 다른 이미지를 통한 시각화로 성능을 검증해봐야겠다.

시각화.... 구글링 해봤지만 뚜렷한 방법은 나오지 않는다. 그래서 gpt에게 물어봐도 keras나 tensorflow를 사용하라고 한다...

그러던 중 gpt에게 pytorch로 짜달라고 했는데 계속 저장된 모델을 가져와서 사용하게 하는 것이다. 그래서 이미 훈련된 모델 있으니까 그대로 사용하도록 코드 짜달라니까

저장한 다음 다시 가져오는 게 안전하다고 했다.

#### 그래서 결과적으로 탄생한 코드는 아래와 같다.

```py
torch.save(model, 'ResNet18.pth')
```

- 모델 저장은 위해서는 한번만 실행

```py
model = torch.load('./Saved_Models/ResNet18.pth')
```

- 이건 한번 실행하면 굳이 다음부턴 실행할 필요 없음

```py
img = Image.open("./Model_Predict_Test_Images/20201211_General_122_DOC_A_M50_MM_069_0101.jpg")
img_resize = img.resize((224, 224))
img_resize
```

- 어떤 사진인지 보기 위해서 가져와서 사이지에 맞게 resize했다.

```py
import torch
import torchvision.transforms as transforms
from PIL import Image

# Define the transforms for preprocessing the image
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the image
img = Image.open("./Model_Predict_Test_Images/20201211_General_122_DOC_A_M50_MM_069_0101.jpg").convert('RGB')

# Preprocess the image
img_tensor = transform(img).unsqueeze(0)

# Move the model and input tensor to the CPU or GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
img_tensor = img_tensor.to(device)
model.to(device)

# Set the model to evaluation mode
model.eval()

# Make a prediction
with torch.no_grad():
    predictions = model(img_tensor)

# Print the predicted class
predicted_class = torch.argmax(predictions[0]).item()
print("Predicted class:", predicted_class)
img_resize
```

- 새로 사진을 가져와서 실행한 저장했던 모델을 통해 결과물을 출력한다.

![image](https://user-images.githubusercontent.com/84713532/228446722-6a811344-8395-49dc-ab06-af2b15b066b1.png)

예측 결과: backswingtop / 정답: backswing

#### 이 외 3번개의 이미지로 더 실험함. 여자 이미지로 훈련된 모델에 남자를 넣어봤지만 크게 상관은 없는 것으로 나타났다.

#### 4개중 3개 정답
