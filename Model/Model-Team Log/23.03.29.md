#### ResNet 성능 검토에 따른 고찰과 모델 예측 결과까지 시각화를 통해 확인할 수 있었다. 이제 VGGNet에 대해 알아보고 성능을 비교해보겠다.

### VGGNet pretrained=True

```py
import torch
model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)
# or any of these variants
# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)
# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)
# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)
# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)
# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)
# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)
# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)
model.eval()
```

- ResNet과 같이 pytorch 공식 홈페이지에서 전이 학습을 위한 코드를 가져왔다. 이것도 pretrain 여부를 구분해서 성능을 검토해볼 예정이다.
- VGGNet은 특징이 층의 깊이와 bn(batch normalization) 여부에서 차이를 볼 수 있다. 모두 실행시킬 필요는 없을 것 같고, 성능의 변화 과정을 살펴보며 규칙을 알아내자.


#### vgg11

```
Epoch 1/10
train | Loss: nan Acc: 0.1635
test | Loss: nan Acc: 0.1000
Epoch 2/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 3/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 4/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 5/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 6/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 7/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 8/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 9/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 10/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
<__main__.TrainModel at 0x7f4fc82cbe80>
```

```
test | Loss: nan Acc: 0.2083
```

- 성능이 너무 안 좋다... 적은 층은 볼 거 없는 거 같고, 유명한 vgg16과 19만 실행시켜봐야겠다.
- 그리고 오차는 nan(not a number)이라고 한다. 무엇? 정확률도 상승하지 않는 것도 이상하다.
- 코드가 ResNet만을 위해서 짜여진 건지 검토해봤지만 아니었다. 문제가 뭔진 모르겠지만 다른 것도 실행해봐야겠다.


#### vgg16

```
Epoch 1/10
train | Loss: nan Acc: 0.1635
test | Loss: nan Acc: 0.1000
Epoch 2/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 3/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 4/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 5/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 6/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 7/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 8/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 9/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
Epoch 10/10
train | Loss: nan Acc: 0.1698
test | Loss: nan Acc: 0.1000
<__main__.TrainModel at 0x7f4fc83944c0>
```

```
test | Loss: nan Acc: 0.2083
```

- 아 이유 알았다. gradient vanishing이다. 그래서 성능의 진전이 없고 오차가 nan 값으로 나온 것이었다.
- 그래서 ResNet의 shortcut connection이 이 문제점을 보완해주며 더 뛰어난 성능을 야기했다.
- 더 볼 것도 없다.

## VGGNet Cut!!!!
