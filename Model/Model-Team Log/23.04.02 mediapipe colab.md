#### 오랜만이다. 그동안 모델을 훈련시킨 결과를 가지고 가설을 세워 그를 해결하고자 노력해왔다.
#### 남은 시간이 얼마 남지 않아 결과물을 내는 것을 우선으로 하고자 결과 이미지를 비교하는 코드를 짜보려 한다.

- 동엽님께서 이전부터 하고 계신 것이 있었다. 그 정보를 바탕으로 해보겠다.

## Mediapipe 씌우기

```py
!pip install mediapipe
```

```
Upload any image that that has a person. We take two example images from the web: https://unsplash.com/photos/v4zceVZ5HK8 and https://unsplash.com/photos/e_rhazQLaSs.
```

```py
from google.colab import files
uploaded = files.upload()
```

```py
import cv2
from google.colab.patches import cv2_imshow
import math
import numpy as np

DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480
def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)

# Read images with OpenCV.
images = {name: cv2.imread(name) for name in uploaded.keys()}
# Preview the images.
for name, image in images.items():
  print(name)   
  resize_and_show(image)
```

![image](https://user-images.githubusercontent.com/84713532/229341861-a34db143-80d1-40a3-a9af-41e57e90f041.png)

```
All MediaPipe Solutions Python API examples are under mp.solutions.

For the MediaPipe Pose solution, we can access this module as `mp_pose = mp.solutions.pose`.

You may change the parameters, such as `static_image_mode` and `min_detection_confidence`, during the initialization. Run `help(mp_pose.Pose)` to get more informations about the parameters.
```

```py
import mediapipe as mp
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils 
mp_drawing_styles = mp.solutions.drawing_styles

help(mp_pose.Pose)
```

```py
# Run MediaPipe Pose and draw pose landmarks.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    # Convert the BGR image to RGB and process it with MediaPipe Pose.
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    # Print nose landmark.
    image_hight, image_width, _ = image.shape
    if not results.pose_landmarks:
      continue
    print(
      f'Nose coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_hight})'
    )

    # Draw pose landmarks.
    print(f'Pose landmarks of {name}:')
    annotated_image = image.copy()
    mp_drawing.draw_landmarks(
        annotated_image,
        results.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
    resize_and_show(annotated_image)
```

![image](https://user-images.githubusercontent.com/84713532/229341879-14dd2dfd-4398-45a9-ac59-4c1a5e206deb.png)

```py
# Run MediaPipe Pose and plot 3d pose world landmarks.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Print the real-world 3D coordinates of nose in meters with the origin at
    # the center between hips.
    print('Nose world landmark:'),
    print(results.pose_world_landmarks.landmark[mp_pose.PoseLandmark.NOSE])
    
    # Plot pose world landmarks.
    mp_drawing.plot_landmarks(
        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)
```

![image](https://user-images.githubusercontent.com/84713532/229341894-15e75fa3-a826-4dc2-bff3-6d2518adc1f7.png)

```py
# Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, 
    model_complexity=2, enable_segmentation=True) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw pose segmentation.
    print(f'Pose segmentation of {name}:')
    annotated_image = image.copy()
    red_img = np.zeros_like(annotated_image, dtype=np.uint8)
    red_img[:, :] = (255,255,255)
    segm_2class = 0.2 + 0.8 * results.segmentation_mask
    segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)
    annotated_image = annotated_image * segm_2class + red_img * (1 - segm_2class)
    resize_and_show(annotated_image)
```

![image](https://user-images.githubusercontent.com/84713532/229341908-7093c96f-6b82-4a72-937a-884926bb1409.png)

#### mediapipe 연결에 배경 없애는 것까지 할 수 있었다. 이제 colab이 아닌 다른 곳에서도 실행시킬 수 있어야 한다.

문제는 

```py
from google.colab import files
uploaded = files.upload()
```

```py
from google.colab.patches import cv2_imshow
```

이 코드들이다. 코랩에서만 사용되는 모듈이기에 바깥에서 사용할 수 있도록 변수명들까지 바꿔줘야 한다.

이걸 성공하고 나면 이미지 2개를 업로드 시켜서 landmark 간의 차이를 비교하도록 해야겠다. landmark도 3차원으로 나올 것 같은데 잘 검토해서 2차원으로 사용하도록 해야겠다.
